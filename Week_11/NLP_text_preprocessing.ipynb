{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/john/School/Sem5/DA/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/john/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/john/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/john/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/john/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "#Natural Language Toolkit) is a leading platform for building Python programs to work with human language data.\n",
    "import nltk \n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The car is driven on the road. The truck is driven on the highway. The bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The car is driven on the road.'\n",
    "d2 = 'The truck is driven on the highway.'\n",
    "d3 = 'The bicycle is driven on the bicycle path.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road. the truck is driven on the highway. the bicycle is driven on the bicycle path.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation ( !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string.punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the car is driven on the road the truck is driven on the highway the bicycle is driven on the bicycle path'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"string.punctuation: \" + string.punctuation);\n",
    "\n",
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'this', 'whom', 'a', 'have', 'by', 'didn', \"isn't\", \"you're\", 'some', 'myself', 'during', 'who', 'not', 'weren', 'only', 'i', \"won't\", 's', 'on', 'is', 'before', 'she', 'those', 'his', 'between', \"hasn't\", 'below', 'each', 'yours', 'don', \"mightn't\", 'd', 'its', 'with', 'shouldn', \"didn't\", 'y', 'does', 'you', 'couldn', \"you'd\", 're', 'it', 'has', 'through', 'theirs', \"wouldn't\", 'wasn', 'about', \"doesn't\", \"you'll\", 'ain', 'yourself', 'when', 'few', 'where', 'such', 'mustn', 'be', 'hadn', 'themselves', \"needn't\", 'aren', 'after', 'having', 'll', 'any', 'these', 'down', 'was', 'what', 'into', 'mightn', 'again', 'for', 'if', 've', 'been', 'but', 'o', 'over', 'the', 'm', 'are', 'just', 'both', 'no', 'same', 'too', 'did', 'of', 'against', 'doing', 'am', 'so', 'there', 'that', 'very', 'wouldn', 'haven', 'ours', 'himself', 'out', 'him', 'why', \"mustn't\", 'nor', \"couldn't\", 'or', 'which', 'in', 'while', 'here', 'how', \"shan't\", 'my', 'me', 'because', 'other', 'won', 'itself', 'isn', 'and', 'as', \"that'll\", 'to', 'will', 'we', 'under', 'should', 'had', 'most', \"should've\", 'can', 'now', 'further', 'doesn', 'herself', \"weren't\", 't', 'all', 'ma', 'from', 'an', 'at', 'yourselves', 'them', 'up', \"aren't\", 'he', 'shan', 'our', 'being', 'off', \"she's\", 'her', 'hers', 'above', 'were', \"wasn't\", 'more', 'own', 'than', 'ourselves', 'then', \"hadn't\", 'hasn', 'do', 'once', \"don't\", \"you've\", \"shouldn't\", \"haven't\", 'your', 'until', 'needn', 'their', \"it's\", 'they'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization (Zusammenfassen verschiedener flektierter Formen desselben Wortes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['car', 'driven', 'road', 'truck', 'driven', 'highway', 'bicycle', 'driven', 'bicycle', 'path'] \n",
      "\n",
      "After lemmatization:\n",
      "['car', 'drive', 'road', 'truck', 'drive', 'highway', 'bicycle', 'drive', 'bicycle', 'path']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['car drive road', \n",
    "          'truck drive highway', \n",
    "          'bicycle drive bicycle path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle  car  drive  highway  path  road  truck\n",
      "0        0    1      1        0     0     1      0\n",
      "1        0    0      1        1     0     0      1\n",
      "2        2    0      1        0     1     0      0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle drive  bicycle path  car drive  drive bicycle  drive highway  \\\n",
      "0              0             0          1              0              0   \n",
      "1              0             0          0              0              1   \n",
      "2              1             1          0              1              0   \n",
      "\n",
      "   drive road  truck drive  \n",
      "0           1            0  \n",
      "1           0            1  \n",
      "2           0            0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 7 \n",
      "\n",
      "The words in the corpus: \n",
      " {'drive', 'highway', 'car', 'path', 'truck', 'road', 'bicycle'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "    drive  highway     car  path   truck    road  bicycle\n",
      "0  0.3333   0.0000  0.3333  0.00  0.0000  0.3333      0.0\n",
      "1  0.3333   0.3333  0.0000  0.00  0.3333  0.0000      0.0\n",
      "2  0.2500   0.0000  0.0000  0.25  0.0000  0.0000      0.5\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "          drive:        0.0\n",
      "        highway:     0.4771\n",
      "            car:     0.4771\n",
      "           path:     0.4771\n",
      "          truck:     0.4771\n",
      "           road:     0.4771\n",
      "        bicycle:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   drive  highway    car    path  truck   road  bicycle\n",
      "0    0.0    0.000  0.159  0.0000  0.000  0.159   0.0000\n",
      "1    0.0    0.159  0.000  0.0000  0.159  0.000   0.0000\n",
      "2    0.0    0.000  0.000  0.1193  0.000  0.000   0.2386\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''European authorities fined Google a record $5.1 \n",
    "          billion on Wednesday for abusing its power in the \n",
    "          mobile phone market.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW11 - Task b - defining own documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_10: I love to travel and explore new destinations. Flying over Japan was an incredible experience. Zurich is a charming city with a rich cultural heritage.\n",
      "\n",
      "1) text to lowercase\n",
      "corpus_11: i love to travel and explore new destinations. flying over japan was an incredible experience. zurich is a charming city with a rich cultural heritage.\n",
      "2) remove punctuation\n",
      "i love to travel and explore new destinations flying over japan was an incredible experience zurich is a charming city with a rich cultural heritage\n",
      "3) Stopwords\n",
      "List of english stopwords: \n",
      "\n",
      "{'this', 'whom', 'a', 'have', 'by', 'didn', \"isn't\", \"you're\", 'some', 'myself', 'during', 'who', 'not', 'weren', 'only', 'i', \"won't\", 's', 'on', 'is', 'before', 'she', 'those', 'his', 'between', \"hasn't\", 'below', 'each', 'yours', 'don', \"mightn't\", 'd', 'its', 'with', 'shouldn', \"didn't\", 'y', 'does', 'you', 'couldn', \"you'd\", 're', 'it', 'has', 'through', 'theirs', \"wouldn't\", 'wasn', 'about', \"doesn't\", \"you'll\", 'ain', 'yourself', 'when', 'few', 'where', 'such', 'mustn', 'be', 'hadn', 'themselves', \"needn't\", 'aren', 'after', 'having', 'll', 'any', 'these', 'down', 'was', 'what', 'into', 'mightn', 'again', 'for', 'if', 've', 'been', 'but', 'o', 'over', 'the', 'm', 'are', 'just', 'both', 'no', 'same', 'too', 'did', 'of', 'against', 'doing', 'am', 'so', 'there', 'that', 'very', 'wouldn', 'haven', 'ours', 'himself', 'out', 'him', 'why', \"mustn't\", 'nor', \"couldn't\", 'or', 'which', 'in', 'while', 'here', 'how', \"shan't\", 'my', 'me', 'because', 'other', 'won', 'itself', 'isn', 'and', 'as', \"that'll\", 'to', 'will', 'we', 'under', 'should', 'had', 'most', \"should've\", 'can', 'now', 'further', 'doesn', 'herself', \"weren't\", 't', 'all', 'ma', 'from', 'an', 'at', 'yourselves', 'them', 'up', \"aren't\", 'he', 'shan', 'our', 'being', 'off', \"she's\", 'her', 'hers', 'above', 'were', \"wasn't\", 'more', 'own', 'than', 'ourselves', 'then', \"hadn't\", 'hasn', 'do', 'once', \"don't\", \"you've\", \"shouldn't\", \"haven't\", 'your', 'until', 'needn', 'their', \"it's\", 'they'}\n",
      "4) toenization and remoal of stopwords\n",
      "['love', 'travel', 'explore', 'new', 'destinations', 'flying', 'japan', 'incredible', 'experience', 'zurich', 'charming', 'city', 'rich', 'cultural', 'heritage']Before lemmatization:\n",
      "['love', 'travel', 'explore', 'new', 'destinations', 'flying', 'japan', 'incredible', 'experience', 'zurich', 'charming', 'city', 'rich', 'cultural', 'heritage'] \n",
      "\n",
      "After lemmatization:\n",
      "['love', 'travel', 'explore', 'new', 'destinations', 'fly', 'japan', 'incredible', 'experience', 'zurich', 'charm', 'city', 'rich', 'cultural', 'heritage']"
     ]
    }
   ],
   "source": [
    "# Defining documents (=sentences)\n",
    "e1 = 'I love to travel and explore new destinations.'\n",
    "e2 = 'Flying over Japan was an incredible experience.'\n",
    "e3 = 'Zurich is a charming city with a rich cultural heritage.'\n",
    "\n",
    "corpus_10 = e1 + ' ' + e2 + ' ' + e3\n",
    "print(\"corpus_10: \" + corpus_10)\n",
    "print()\n",
    "\n",
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "print(\"1) text to lowercase\")\n",
    "corpus_11 = text_lowercase(corpus_10)\n",
    "print(\"corpus_11: \" + corpus_11)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"string.punctuation: \" + string.punctuation);\n",
    "\n",
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "print(\"2) remove punctuation\")\n",
    "corpus_13 = remove_punctuation(corpus_11)\n",
    "print(corpus_13)\n",
    "\n",
    "\n",
    "print(\"3) Stopwords\")\n",
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\", '\\n')\n",
    "print(eng_stopwords)\n",
    "\n",
    "\n",
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "print(\"4) toenization and remoal of stopwords\")\n",
    "corpus_14 = remove_stopwords(corpus_13)\n",
    "print(corpus_14, end=\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_14:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_15 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_14, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_15, end=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.c Redefine the text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus2 = ['love travel explore new destination', \n",
    "          'fly japan incredible experience', \n",
    "          'zurich charm city rich cultural heritage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.d create a document-term matrix with ngram_range=(1,1)  and a document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle  car  drive  highway  path  road  truck\n",
      "0        0    1      1        0     0     1      0\n",
      "1        0    0      1        1     0     0      1\n",
      "2        2    0      1        0     1     0      0\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   bicycle drive  bicycle path  car drive  drive bicycle  drive highway  \\\n",
      "0              0             0          1              0              0   \n",
      "1              0             0          0              0              1   \n",
      "2              1             1          0              1              0   \n",
      "\n",
      "   drive road  truck drive  \n",
      "0           1            0  \n",
      "1           0            1  \n",
      "2           0            0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.e\n",
    "Based on the 'corpus' from c) create a:\n",
    "<p>-Term Frequency (TF) matrix</p>\n",
    "<p>-Inverse Document Frequency (IDF) matrix</p>\n",
    "<p>-Term Frequency - Inverse Documetn Frequency (TF-IDF) matrix</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 7 \n",
      "\n",
      "The words in the corpus: \n",
      " {'drive', 'highway', 'car', 'path', 'truck', 'road', 'bicycle'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "    drive  highway     car  path   truck    road  bicycle\n",
      "0  0.3333   0.0000  0.3333  0.00  0.0000  0.3333      0.0\n",
      "1  0.3333   0.3333  0.0000  0.00  0.3333  0.0000      0.0\n",
      "2  0.2500   0.0000  0.0000  0.25  0.0000  0.0000      0.5\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "          drive:        0.0\n",
      "        highway:     0.4771\n",
      "            car:     0.4771\n",
      "           path:     0.4771\n",
      "          truck:     0.4771\n",
      "           road:     0.4771\n",
      "        bicycle:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   drive  highway    car    path  truck   road  bicycle\n",
      "0    0.0    0.000  0.159  0.0000  0.000  0.159   0.0000\n",
      "1    0.0    0.159  0.000  0.0000  0.159  0.000   0.0000\n",
      "2    0.0    0.000  0.000  0.1193  0.000  0.000   0.2386\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.f (part-of_speach POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'NNP', 'O'),\n",
      " ('Inc.', 'NNP', 'O'),\n",
      " ('unveiled', 'VBD', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('latest', 'JJS', 'O'),\n",
      " ('iPhone', 'NN', 'B-NP'),\n",
      " ('model', 'NN', 'B-NP'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Tuesday', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('creating', 'VBG', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('buzz', 'NN', 'I-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('tech', 'NN', 'I-NP'),\n",
      " ('community', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('The', 'DT', 'B-NP'),\n",
      " ('new', 'JJ', 'I-NP'),\n",
      " ('device', 'NN', 'I-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('equipped', 'VBN', 'O'),\n",
      " ('with', 'IN', 'O'),\n",
      " ('cutting-edge', 'NN', 'B-NP'),\n",
      " ('features', 'NNS', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('expected', 'VBN', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('dominate', 'VB', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('smartphone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Apple Inc. unveiled its latest iPhone model on Tuesday, \n",
    "          creating a buzz in the tech community. The new device, \n",
    "          equipped with cutting-edge features, is expected to \n",
    "          dominate the smartphone market.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Explanation of POS Tags -->\n",
    "\n",
    "<p>('to', 'TO', 'O'): The word \"to\" is a word that often shows direction or intention. In this case, it is not part of a special group of words (Named Entity) in the sentence.</p>\n",
    "\n",
    "<p>('dominate', 'VB', 'O'): The word \"dominate\" is an action word. It means to be in control. Here, it's not part of any special group of words in the sentence.</p>\n",
    "\n",
    "<p>('the', 'DT', 'B-NP'): The word \"the\" is a word we use to talk about something specific. In this sentence, it's the beginning of a group of words (phrase) that is about something.</p>\n",
    "\n",
    "<p>('smartphone', 'NN', 'I-NP'): The word \"smartphone\" is a technical word for a type of mobile phone. It is part of the same group of words (phrase) as \"the,\" and it follows it.</p>\n",
    "\n",
    "<p>('market', 'NN', 'B-NP'): The word \"market\" is another word for a place where things are bought and sold. It is part of a new group of words (phrase) that is about something different from the smartphone.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The Parts Of Speech Tag List -->\n",
    "\n",
    "<!-- In the above example, the output contained tags like NN, NNP, VBD, etc. Following is the complete list of such POS tags. -->\n",
    "\n",
    "<p>CC Coordinating Conjunction</p>\n",
    "<p>CD Cardinal Digit</p>\n",
    "<p>DT Determiner</p>\n",
    "<p>EX Existential There. Example: “there is” … think of it like “there exists”)</p>\n",
    "<p>FW Foreign Word.</p>\n",
    "<p>IN Preposition/Subordinating Conjunction.</p>\n",
    "<p>JJ Adjective.</p>\n",
    "<p>JJR Adjective, Comparative.</p>\n",
    "<p>JJS Adjective, Superlative.</p>\n",
    "<p>LS List Marker 1.</p>\n",
    "<p>MD Modal.</p>\n",
    "<p>NN Noun, Singular.</p>\n",
    "<p>NNS Noun Plural.</p>\n",
    "<p>NNP Proper Noun, Singular.</p>\n",
    "<p>NNPS Proper Noun, Plural.</p>\n",
    "<p>PDT Predeterminer.</p>\n",
    "<p>POS Possessive Ending. Example: parent’s</p>\n",
    "<p>PRP Personal Pronoun. Examples: I, he, she</p>\n",
    "<p>PRP$ Possessive Pronoun. Examples: my, his, hers</p>\n",
    "<p>RB Adverb. Examples: very, silently,</p>\n",
    "<p>RBR Adverb, Comparative. Example: better</p>\n",
    "<p>RBS Adverb, Superlative. Example: best</p>\n",
    "<p>RP Particle. Example: give up</p>\n",
    "<p>TO to. Example: go ‘to’ the store.</p>\n",
    "<p>UH Interjection. Example: errrrrrrrm</p>\n",
    "<p>VB Verb, Base Form. Example: take</p>\n",
    "<p>VBD Verb, Past Tense. Example: took</p>\n",
    "<p>VBG Verb, Gerund/Present Participle. Example: taking</p>\n",
    "<p>VBN Verb, Past Participle. Example: taken</p>\n",
    "<p>VBP Verb, Sing Present, non-3d take</p>\n",
    "<p>VBZ Verb, 3rd person sing. present takes</p>\n",
    "<p>WDT wh-determiner. Example: which</p>\n",
    "<p>WP wh-pronoun. Example: who, what</p>\n",
    "<p>WP$ possessive wh-pronoun. Example: whose</p>\n",
    "<p>WRB wh-abverb. Example: where, when</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Darwin | 23.0.0\n",
      "Datetime: 2023-12-08 17:06:18\n",
      "Python Version: 3.9.6\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
